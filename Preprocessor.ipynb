{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4764a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from transformers import pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fa93fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sampleinput.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dataset_path = data[\"path\"]\n",
    "target_column = data[\"target\"]\n",
    "LOG_FILE = \"Output.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e9bdcb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(e):\n",
    "    try:\n",
    "        with open(LOG_FILE, \"a\") as f:\n",
    "            json.dump({\"error\": str(e)}, f)\n",
    "            f.write(\"\\n\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "36682a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Dataset = pd.read_csv(dataset_path)\n",
    "except Exception as e:\n",
    "    log_error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e797d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=Dataset.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3a3a643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numprompt=len(data)-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ed4758ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8dc6408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\n",
    "    \"drop or remove columns from dataset\",             \n",
    "    \"fill missing values with mean\",                   \n",
    "    \"fill missing values with median\",                  \n",
    "    \"fill missing values with mode\",                   \n",
    "    \"remove duplicate rows from dataset\",             \n",
    "    \"convert or fix data types of columns\",           \n",
    "    \"standardize or scale numeric features\",         \n",
    "    \"normalize numeric features to range [0,1]\",       \n",
    "    \"encode categorical features (one-hot or label)\", \n",
    "    \"reduce dimensions using PCA or similar methods\",  \n",
    "    \"filter rows based on conditions or criteria\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a80b59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_columns_from_text(user_input):\n",
    "    lower_input = user_input.lower()\n",
    "    return [col for col in columns if col.lower() in lower_input]\n",
    "\n",
    "def drop_columns(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        Dataset.drop(columns=cols, inplace=True)\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def fill_missing_mean(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        Dataset[cols] = Dataset[cols].fillna(Dataset[cols].mean())\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def fill_missing_median(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        Dataset[cols] = Dataset[cols].fillna(Dataset[cols].median())\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def fill_missing_mode(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        for col in cols:\n",
    "            Dataset[col].fillna(Dataset[col].mode()[0], inplace=True)\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def remove_duplicates(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        Dataset.drop_duplicates(inplace=True)\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def fix_data_types(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        dtype_dict = {}\n",
    "        user_lower = user_input.lower()\n",
    "        for col in columns:\n",
    "            if f\"{col} to int\" in user_lower:\n",
    "                dtype_dict[col] = int\n",
    "            elif f\"{col} to float\" in user_lower:\n",
    "                dtype_dict[col] = float\n",
    "            elif f\"{col} to str\" in user_lower:\n",
    "                dtype_dict[col] = str\n",
    "            elif f\"{col} to date\" in user_lower or f\"{col} to datetime\" in user_lower:\n",
    "                dtype_dict[col] = \"datetime\"\n",
    "\n",
    "        for col, dtype in dtype_dict.items():\n",
    "            if dtype == \"datetime\":\n",
    "                Dataset[col] = pd.to_datetime(Dataset[col], errors=\"coerce\")\n",
    "            else:\n",
    "                Dataset[col] = Dataset[col].astype(dtype)\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "\n",
    "def standardize_columns(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        if not cols:\n",
    "            cols = Dataset.select_dtypes(include='number').columns.tolist()\n",
    "            if \"target\" in cols:\n",
    "                cols.remove(\"target\")\n",
    "        scaler = StandardScaler()\n",
    "        Dataset[cols] = scaler.fit_transform(Dataset[cols])\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "\n",
    "def normalize_columns(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        if not cols:\n",
    "            cols = Dataset.select_dtypes(include='number').columns.tolist()\n",
    "            if \"target\" in cols:\n",
    "                cols.remove(\"target\")\n",
    "        scaler = MinMaxScaler()\n",
    "        Dataset[cols] = scaler.fit_transform(Dataset[cols])\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "\n",
    "def encode_categorical(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        le = LabelEncoder()\n",
    "        for col in cols:\n",
    "            Dataset[col] = le.fit_transform(Dataset[col])\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def reduce_dimensions(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        numeric_cols = [col for col in cols if col in Dataset.select_dtypes(include='number').columns]\n",
    "\n",
    "        if not numeric_cols:\n",
    "            numeric_cols = Dataset.select_dtypes(include='number').columns.tolist()\n",
    "            if \"target\" in numeric_cols:\n",
    "                numeric_cols.remove(\"target\")\n",
    "\n",
    "        n_components = 2\n",
    "        pca = PCA(n_components=n_components)\n",
    "        reduced = pca.fit_transform(Dataset[numeric_cols])\n",
    "        for i in range(n_components):\n",
    "            Dataset[f\"PCA_{i+1}\"] = reduced[:, i]\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "\n",
    "def filter_rows(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        filtered_df = Dataset.query(user_input)\n",
    "        Dataset.drop(Dataset.index, inplace=True)\n",
    "        for col in filtered_df.columns:\n",
    "            Dataset[col] = filtered_df[col]\n",
    "    except Exception as e:\n",
    "        log_error(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9cb04b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_function_mapping = {\n",
    "    \"drop or remove columns from dataset\": drop_columns,\n",
    "    \"fill missing values with mean\": fill_missing_mean,\n",
    "    \"fill missing values with median\": fill_missing_median,\n",
    "    \"fill missing values with mode\": fill_missing_mode,\n",
    "    \"remove duplicate rows from dataset\": remove_duplicates,\n",
    "    \"convert or fix data types of columns\": fix_data_types,\n",
    "    \"standardize or scale numeric features\": standardize_columns,\n",
    "    \"normalize numeric features to range [0,1]\": normalize_columns,\n",
    "    \"encode categorical features (one-hot or label)\": encode_categorical,\n",
    "    \"reduce dimensions using PCA or similar methods\": reduce_dimensions,\n",
    "    \"filter rows based on conditions or criteria\": filter_rows\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d9ce2f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'label all columns', 'labels': ['convert or fix data types of columns', 'standardize or scale numeric features', 'encode categorical features (one-hot or label)', 'filter rows based on conditions or criteria', 'fill missing values with mean', 'fill missing values with median', 'remove duplicate rows from dataset', 'fill missing values with mode', 'normalize numeric features to range [0,1]', 'drop or remove columns from dataset', 'reduce dimensions using PCA or similar methods'], 'scores': [0.3298444151878357, 0.2231069803237915, 0.17421609163284302, 0.06572872400283813, 0.0439617782831192, 0.04132398962974548, 0.03972584754228592, 0.036816686391830444, 0.024962954223155975, 0.012689097784459591, 0.007623337674885988]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, numprompt):\n",
    "    user_input = data[f\"prompt_{i+1}\"]\n",
    "    result = classifier(user_input, candidate_labels)\n",
    "    print(result)\n",
    "    action = result['labels'][0]\n",
    "    if action in intent_function_mapping:\n",
    "        intent_function_mapping[action](user_input)\n",
    "    else:\n",
    "        log_error(f\"Unrecognized action: {action}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a5463526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'label all columns',\n",
       " 'labels': ['convert or fix data types of columns',\n",
       "  'standardize or scale numeric features',\n",
       "  'encode categorical features (one-hot or label)',\n",
       "  'filter rows based on conditions or criteria',\n",
       "  'fill missing values with mean',\n",
       "  'fill missing values with median',\n",
       "  'remove duplicate rows from dataset',\n",
       "  'fill missing values with mode',\n",
       "  'normalize numeric features to range [0,1]',\n",
       "  'drop or remove columns from dataset',\n",
       "  'reduce dimensions using PCA or similar methods'],\n",
       " 'scores': [0.3298444151878357,\n",
       "  0.2231069803237915,\n",
       "  0.17421609163284302,\n",
       "  0.06572872400283813,\n",
       "  0.0439617782831192,\n",
       "  0.04132398962974548,\n",
       "  0.03972584754228592,\n",
       "  0.036816686391830444,\n",
       "  0.024962954223155975,\n",
       "  0.012689097784459591,\n",
       "  0.007623337674885988]}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8c01144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame.time</th>\n",
       "      <th>frame.len</th>\n",
       "      <th>frame.protocols</th>\n",
       "      <th>eth.src</th>\n",
       "      <th>eth.dst</th>\n",
       "      <th>ip.dst</th>\n",
       "      <th>ip.src</th>\n",
       "      <th>ip.flags</th>\n",
       "      <th>ip.ttl</th>\n",
       "      <th>ip.proto</th>\n",
       "      <th>...</th>\n",
       "      <th>tcp.dstport</th>\n",
       "      <th>tcp.flags</th>\n",
       "      <th>tcp.window_size_value</th>\n",
       "      <th>tcp.window_size_scalefactor</th>\n",
       "      <th>tcp.checksum</th>\n",
       "      <th>tcp.options</th>\n",
       "      <th>tcp.pdu.size</th>\n",
       "      <th>udp.srcport</th>\n",
       "      <th>udp.dstport</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan 14, 2025 18:40:22.447710000 GMT</td>\n",
       "      <td>67</td>\n",
       "      <td>eth:ethertype:ip:udp:dns</td>\n",
       "      <td>02:42:52:d7:fa:00</td>\n",
       "      <td>0c:6e:9c:16:00:00</td>\n",
       "      <td>192.168.0.2</td>\n",
       "      <td>192.168.18.17</td>\n",
       "      <td>0x02</td>\n",
       "      <td>64</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48322.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan 14, 2025 18:40:22.453402000 GMT</td>\n",
       "      <td>83</td>\n",
       "      <td>eth:ethertype:ip:udp:dns</td>\n",
       "      <td>0c:6e:9c:16:00:00</td>\n",
       "      <td>02:42:52:d7:fa:00</td>\n",
       "      <td>192.168.18.17</td>\n",
       "      <td>192.168.0.2</td>\n",
       "      <td>0x02</td>\n",
       "      <td>61</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>48322.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan 14, 2025 18:40:22.453507000 GMT</td>\n",
       "      <td>90</td>\n",
       "      <td>eth:ethertype:ip:udp:ntp</td>\n",
       "      <td>02:42:52:d7:fa:00</td>\n",
       "      <td>0c:6e:9c:16:00:00</td>\n",
       "      <td>192.168.0.3</td>\n",
       "      <td>192.168.18.17</td>\n",
       "      <td>0x02</td>\n",
       "      <td>64</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46343.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan 14, 2025 18:40:22.458119000 GMT</td>\n",
       "      <td>90</td>\n",
       "      <td>eth:ethertype:ip:udp:ntp</td>\n",
       "      <td>0c:6e:9c:16:00:00</td>\n",
       "      <td>02:42:52:d7:fa:00</td>\n",
       "      <td>192.168.18.17</td>\n",
       "      <td>192.168.0.3</td>\n",
       "      <td>0x02</td>\n",
       "      <td>61</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.0</td>\n",
       "      <td>46343.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan 14, 2025 18:40:22.560013000 GMT</td>\n",
       "      <td>76</td>\n",
       "      <td>eth:ethertype:ip:udp:dns</td>\n",
       "      <td>02:42:52:d7:fa:00</td>\n",
       "      <td>0c:6e:9c:16:00:00</td>\n",
       "      <td>192.168.0.2</td>\n",
       "      <td>192.168.18.17</td>\n",
       "      <td>0x02</td>\n",
       "      <td>64</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36848.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            frame.time  frame.len           frame.protocols  \\\n",
       "0  Jan 14, 2025 18:40:22.447710000 GMT         67  eth:ethertype:ip:udp:dns   \n",
       "1  Jan 14, 2025 18:40:22.453402000 GMT         83  eth:ethertype:ip:udp:dns   \n",
       "2  Jan 14, 2025 18:40:22.453507000 GMT         90  eth:ethertype:ip:udp:ntp   \n",
       "3  Jan 14, 2025 18:40:22.458119000 GMT         90  eth:ethertype:ip:udp:ntp   \n",
       "4  Jan 14, 2025 18:40:22.560013000 GMT         76  eth:ethertype:ip:udp:dns   \n",
       "\n",
       "             eth.src            eth.dst         ip.dst         ip.src  \\\n",
       "0  02:42:52:d7:fa:00  0c:6e:9c:16:00:00    192.168.0.2  192.168.18.17   \n",
       "1  0c:6e:9c:16:00:00  02:42:52:d7:fa:00  192.168.18.17    192.168.0.2   \n",
       "2  02:42:52:d7:fa:00  0c:6e:9c:16:00:00    192.168.0.3  192.168.18.17   \n",
       "3  0c:6e:9c:16:00:00  02:42:52:d7:fa:00  192.168.18.17    192.168.0.3   \n",
       "4  02:42:52:d7:fa:00  0c:6e:9c:16:00:00    192.168.0.2  192.168.18.17   \n",
       "\n",
       "  ip.flags  ip.ttl  ip.proto  ... tcp.dstport  tcp.flags  \\\n",
       "0     0x02      64        17  ...         NaN        NaN   \n",
       "1     0x02      61        17  ...         NaN        NaN   \n",
       "2     0x02      64        17  ...         NaN        NaN   \n",
       "3     0x02      61        17  ...         NaN        NaN   \n",
       "4     0x02      64        17  ...         NaN        NaN   \n",
       "\n",
       "   tcp.window_size_value  tcp.window_size_scalefactor tcp.checksum  \\\n",
       "0                    NaN                          NaN          NaN   \n",
       "1                    NaN                          NaN          NaN   \n",
       "2                    NaN                          NaN          NaN   \n",
       "3                    NaN                          NaN          NaN   \n",
       "4                    NaN                          NaN          NaN   \n",
       "\n",
       "   tcp.options  tcp.pdu.size udp.srcport udp.dstport   label  \n",
       "0          NaN           NaN     48322.0        53.0  Benign  \n",
       "1          NaN           NaN        53.0     48322.0  Benign  \n",
       "2          NaN           NaN     46343.0       123.0  Benign  \n",
       "3          NaN           NaN       123.0     46343.0  Benign  \n",
       "4          NaN           NaN     36848.0        53.0  Benign  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
