{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder , OneHotEncoder\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "fa93fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sampleinput.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dataset_path = data[\"path\"]\n",
    "target_column = data[\"target\"]\n",
    "LOG_FILE = \"Output.json\"\n",
    "threshold = 0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "e9bdcb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(e):\n",
    "    try:\n",
    "        with open(LOG_FILE, \"a\") as f:\n",
    "            json.dump({\"error\": str(e)}, f)\n",
    "            f.write(\"\\n\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "36682a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Dataset = pd.read_csv(dataset_path)\n",
    "except Exception as e:\n",
    "    log_error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "e797d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=Dataset.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "3a3a643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numprompt=len(data)-2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "25010215",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "10d9bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = [\n",
    "    \"drop columns from dataset\",\n",
    "    \"fill missing values with mean imputation\",\n",
    "    \"fill missing values with median imputation\",\n",
    "    \"fill missing values with mode imputation\",\n",
    "    \"remove duplicate rows\",\n",
    "    \"convert or change data types of columns\",\n",
    "    \"standardize numeric columns using z-score scaling\",\n",
    "    \"normalize numeric columns to range 0 to 1\",\n",
    "    \"encode categorical columns using label encoding\",\n",
    "    \"reduce dataset dimensions with PCA\",\n",
    "    \"filter dataset rows based on conditions\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "82eff688",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embs = embedder.encode(candidate_labels, convert_to_tensor=True)\n",
    "def classifier(user_input, candidate_labels):\n",
    "    input_emb = embedder.encode(user_input, convert_to_tensor=True)\n",
    "\n",
    "    cos_scores = util.cos_sim(input_emb, label_embs)[0]\n",
    "    scores = cos_scores.tolist()\n",
    "\n",
    "    label_scores = list(zip(candidate_labels, scores))\n",
    "\n",
    "    label_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    labels_sorted, scores_sorted = zip(*label_scores)\n",
    "\n",
    "    result = {\n",
    "        \"sequence\": user_input,\n",
    "        \"labels\": list(labels_sorted),\n",
    "        \"scores\": list(scores_sorted)\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "a80b59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_columns_from_text(user_input):\n",
    "    lower_input = user_input.lower()\n",
    "    return [col for col in columns if col.lower() in lower_input]\n",
    "\n",
    "def drop_columns(user_input):\n",
    "    global Dataset, target_column\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        cols = [c for c in cols if c != target_column]\n",
    "        if cols:\n",
    "            Dataset.drop(columns=cols, inplace=True)\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def fill_missing_mean(user_input):\n",
    "    global Dataset, target_column\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        if not cols:\n",
    "            cols = Dataset.select_dtypes(include=\"number\").columns.tolist()\n",
    "            if target_column in cols:\n",
    "                cols.remove(target_column)\n",
    "        Dataset[cols] = Dataset[cols].fillna(Dataset[cols].mean())\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def fill_missing_median(user_input):\n",
    "    global Dataset, target_column\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        if not cols:\n",
    "            cols = Dataset.select_dtypes(include=\"number\").columns.tolist()\n",
    "            if target_column in cols:\n",
    "                cols.remove(target_column)\n",
    "        Dataset[cols] = Dataset[cols].fillna(Dataset[cols].median())\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def fill_missing_mode(user_input):\n",
    "    global Dataset, target_column\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        if not cols:\n",
    "            cols = Dataset.select_dtypes(include=[\"object\", \"category\", \"string\"]).columns.tolist()\n",
    "            if target_column in cols:\n",
    "                cols.remove(target_column)\n",
    "        for col in cols:\n",
    "            Dataset[col].fillna(Dataset[col].mode()[0], inplace=True)\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def remove_duplicates(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        Dataset.drop_duplicates(inplace=True)\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def fix_data_types(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        dtype_dict = {}\n",
    "        user_lower = user_input.lower()\n",
    "        for col in columns:\n",
    "            if f\"{col} to int\" in user_lower:\n",
    "                dtype_dict[col] = int\n",
    "            elif f\"{col} to float\" in user_lower:\n",
    "                dtype_dict[col] = float\n",
    "            elif f\"{col} to str\" in user_lower:\n",
    "                dtype_dict[col] = str\n",
    "            elif f\"{col} to date\" in user_lower or f\"{col} to datetime\" in user_lower:\n",
    "                dtype_dict[col] = \"datetime\"\n",
    "\n",
    "        for col, dtype in dtype_dict.items():\n",
    "            if dtype == \"datetime\":\n",
    "                Dataset[col] = pd.to_datetime(Dataset[col], errors=\"coerce\")\n",
    "            else:\n",
    "                Dataset[col] = Dataset[col].astype(dtype)\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def standardize_columns(user_input):\n",
    "    global Dataset, target_column\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        if not cols:\n",
    "            cols = Dataset.select_dtypes(include=\"number\").columns.tolist()\n",
    "            if target_column in cols:\n",
    "                cols.remove(target_column)\n",
    "        scaler = StandardScaler()\n",
    "        Dataset[cols] = scaler.fit_transform(Dataset[cols])\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def normalize_columns(user_input):\n",
    "    global Dataset, target_column\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        if not cols:\n",
    "            cols = Dataset.select_dtypes(include=\"number\").columns.tolist()\n",
    "            if target_column in cols:\n",
    "                cols.remove(target_column)\n",
    "        scaler = MinMaxScaler()\n",
    "        Dataset[cols] = scaler.fit_transform(Dataset[cols])\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def encode_categorical(user_input):\n",
    "    global Dataset, target_column\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        if not cols:\n",
    "            cols = Dataset.select_dtypes(include=[\"object\", \"category\", \"string\"]).columns.tolist()\n",
    "            if target_column in cols:\n",
    "                cols.remove(target_column)\n",
    "        cols = [c for c in cols if c in Dataset.columns]\n",
    "        for col in cols:\n",
    "            le = LabelEncoder()\n",
    "            Dataset[col] = le.fit_transform(Dataset[col].astype(str))\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def reduce_dimensions(user_input):\n",
    "    global Dataset, target_column\n",
    "    try:\n",
    "        cols = extract_columns_from_text(user_input)\n",
    "        numeric_cols = [col for col in cols if col in Dataset.select_dtypes(include=\"number\").columns]\n",
    "\n",
    "        if not numeric_cols:\n",
    "            numeric_cols = Dataset.select_dtypes(include=\"number\").columns.tolist()\n",
    "            if target_column in numeric_cols:\n",
    "                numeric_cols.remove(target_column)\n",
    "\n",
    "        n_components = 2\n",
    "        pca = PCA(n_components=n_components)\n",
    "        reduced = pca.fit_transform(Dataset[numeric_cols])\n",
    "\n",
    "        for i in range(n_components):\n",
    "            Dataset[f\"PCA_{i+1}\"] = reduced[:, i]\n",
    "    except Exception as e:\n",
    "        log_error(e)\n",
    "\n",
    "def filter_rows(user_input):\n",
    "    global Dataset\n",
    "    try:\n",
    "        filtered_df = Dataset.query(user_input)\n",
    "        Dataset.drop(Dataset.index, inplace=True)\n",
    "        for col in filtered_df.columns:\n",
    "            Dataset[col] = filtered_df[col]\n",
    "    except Exception as e:\n",
    "        log_error(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "9cb04b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_function_mapping = {\n",
    "    \"drop columns from dataset\": drop_columns,\n",
    "    \"fill missing values with mean imputation\": fill_missing_mean,\n",
    "    \"fill missing values with median imputation\": fill_missing_median,\n",
    "    \"fill missing values with mode imputation\": fill_missing_mode,\n",
    "    \"remove duplicate rows\": remove_duplicates,\n",
    "    \"convert or change data types of columns\": fix_data_types,\n",
    "    \"standardize numeric columns using z-score scaling\": standardize_columns,\n",
    "    \"normalize numeric columns to range 0 to 1\": normalize_columns,\n",
    "    \"encode categorical columns using label encoding\": encode_categorical,\n",
    "    \"reduce dataset dimensions with PCA\": reduce_dimensions,\n",
    "    \"filter dataset rows based on conditions\": filter_rows\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "d9ce2f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, numprompt):\n",
    "    user_input = data[f\"prompt_{i+1}\"]\n",
    "    result = classifier(user_input, candidate_labels)\n",
    "    action = result['labels'][0]\n",
    "    if result['scores'][0] >= threshold:\n",
    "        intent_function_mapping[action](user_input)\n",
    "    else:\n",
    "        log_error(f\"Unrecognized prompt: {user_input}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea84f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_dummies = pd.get_dummies(Dataset[target_column], prefix=target_column)\n",
    "\n",
    "Dataset.drop(columns=[target_column], inplace=True)\n",
    "\n",
    "Dataset = pd.concat([Dataset, target_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "8c01144e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame.time</th>\n",
       "      <th>frame.len</th>\n",
       "      <th>frame.protocols</th>\n",
       "      <th>eth.src</th>\n",
       "      <th>eth.dst</th>\n",
       "      <th>ip.dst</th>\n",
       "      <th>ip.src</th>\n",
       "      <th>ip.flags</th>\n",
       "      <th>ip.ttl</th>\n",
       "      <th>ip.proto</th>\n",
       "      <th>...</th>\n",
       "      <th>tcp.checksum</th>\n",
       "      <th>tcp.options</th>\n",
       "      <th>tcp.pdu.size</th>\n",
       "      <th>udp.srcport</th>\n",
       "      <th>udp.dstport</th>\n",
       "      <th>label_Benign</th>\n",
       "      <th>label_Ingress Tool Transfer</th>\n",
       "      <th>label_TCP Scan</th>\n",
       "      <th>label_Telnet Brute Force</th>\n",
       "      <th>label_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>34084</td>\n",
       "      <td>37750</td>\n",
       "      <td>311.646226</td>\n",
       "      <td>48322.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>34084</td>\n",
       "      <td>37750</td>\n",
       "      <td>311.646226</td>\n",
       "      <td>53.0</td>\n",
       "      <td>48322.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>34084</td>\n",
       "      <td>37750</td>\n",
       "      <td>311.646226</td>\n",
       "      <td>46343.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>34084</td>\n",
       "      <td>37750</td>\n",
       "      <td>311.646226</td>\n",
       "      <td>123.0</td>\n",
       "      <td>46343.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>34084</td>\n",
       "      <td>37750</td>\n",
       "      <td>311.646226</td>\n",
       "      <td>36848.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   frame.time  frame.len  frame.protocols  eth.src  eth.dst  ip.dst  ip.src  \\\n",
       "0           0         67                4        1        3       1       6   \n",
       "1           1         83                4        3        1       6       1   \n",
       "2           2         90                5        1        3       2       6   \n",
       "3           3         90                5        3        1       6       2   \n",
       "4           4         76                4        1        3       1       6   \n",
       "\n",
       "   ip.flags  ip.ttl  ip.proto  ...  tcp.checksum  tcp.options  tcp.pdu.size  \\\n",
       "0         1      64        17  ...         34084        37750    311.646226   \n",
       "1         1      61        17  ...         34084        37750    311.646226   \n",
       "2         1      64        17  ...         34084        37750    311.646226   \n",
       "3         1      61        17  ...         34084        37750    311.646226   \n",
       "4         1      64        17  ...         34084        37750    311.646226   \n",
       "\n",
       "   udp.srcport  udp.dstport  label_Benign  label_Ingress Tool Transfer  \\\n",
       "0      48322.0         53.0          True                        False   \n",
       "1         53.0      48322.0          True                        False   \n",
       "2      46343.0        123.0          True                        False   \n",
       "3        123.0      46343.0          True                        False   \n",
       "4      36848.0         53.0          True                        False   \n",
       "\n",
       "   label_TCP Scan  label_Telnet Brute Force  label_Unknown  \n",
       "0           False                     False          False  \n",
       "1           False                     False          False  \n",
       "2           False                     False          False  \n",
       "3           False                     False          False  \n",
       "4           False                     False          False  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "719ce878",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.to_csv(\"Processed_Dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOG_FILE, \"r\") as f:\n",
    "    data=json.load(f)\n",
    "\n",
    "data[\"Processed_dataset_path\"]=\"Processed_Dataset.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
